{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "import flwr as fl\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set client ID (change for each client: 1,2,3,4)\n",
    "CLIENT_ID = 1  # Change this for each client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 15:13:32,724 - Client_1 - INFO - \n",
      "==================== DEVICE INFO ====================\n",
      "2025-03-09 15:13:32,725 - Client_1 - INFO - Client 1 using device: cuda\n",
      "2025-03-09 15:13:32,725 - Client_1 - INFO - ======================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== DEVICE INFO ====================\n",
      "Client 1 using device: cuda\n",
      "======================================================\n"
     ]
    }
   ],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(f\"Client_{CLIENT_ID}\")\n",
    "\n",
    "def log_status(status: str, details: str = \"\"):\n",
    "    status_line = f\"\\n{'='*20} {status} {'='*20}\"\n",
    "    logger.info(status_line)\n",
    "    if details:\n",
    "        logger.info(details)\n",
    "    logger.info(\"=\"*len(status_line))\n",
    "    print(status_line)\n",
    "    if details:\n",
    "        print(details)\n",
    "    print(\"=\"*len(status_line))\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(42 + CLIENT_ID)\n",
    "\n",
    "# Check CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "log_status(\"DEVICE INFO\", f\"Client {CLIENT_ID} using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 15:13:32,754 - Client_1 - INFO - \n",
      "==================== DIRECTORY INFO ====================\n",
      "2025-03-09 15:13:32,755 - Client_1 - INFO - Using temporary directory: C:\\Users\\NILATA~1\\AppData\\Local\\Temp\\tmp9bgt6qn4\n",
      "2025-03-09 15:13:32,755 - Client_1 - INFO - =========================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== DIRECTORY INFO ====================\n",
      "Using temporary directory: C:\\Users\\NILATA~1\\AppData\\Local\\Temp\\tmp9bgt6qn4\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "# Create temporary directory for outputs\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "log_status(\"DIRECTORY INFO\", f\"Using temporary directory: {temp_dir}\")\n",
    "\n",
    "# Define message size\n",
    "GRPC_MAX_MESSAGE_LENGTH = 1024 * 1024 * 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 15:13:32,780 - Client_1 - INFO - \n",
      "==================== LOADING DATASET ====================\n",
      "2025-03-09 15:13:32,782 - Client_1 - INFO - Client 1 loading dataset...\n",
      "2025-03-09 15:13:32,782 - Client_1 - INFO - ==========================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== LOADING DATASET ====================\n",
      "Client 1 loading dataset...\n",
      "==========================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 15:13:34,540 - Client_1 - INFO - \n",
      "==================== DATASET LOADED ====================\n",
      "2025-03-09 15:13:34,542 - Client_1 - INFO - Dataset size: 1000 examples\n",
      "Range: 0 to 1000\n",
      "2025-03-09 15:13:34,543 - Client_1 - INFO - =========================================================\n",
      "2025-03-09 15:13:34,544 - Client_1 - INFO - \n",
      "==================== TEST QUESTIONS SELECTED ====================\n",
      "2025-03-09 15:13:34,544 - Client_1 - INFO - Number of test questions: 5\n",
      "2025-03-09 15:13:34,545 - Client_1 - INFO - ==================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== DATASET LOADED ====================\n",
      "Dataset size: 1000 examples\n",
      "Range: 0 to 1000\n",
      "=========================================================\n",
      "\n",
      "==================== TEST QUESTIONS SELECTED ====================\n",
      "Number of test questions: 5\n",
      "==================================================================\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "log_status(\"LOADING DATASET\", f\"Client {CLIENT_ID} loading dataset...\")\n",
    "try:\n",
    "    dataset = load_dataset(\"medalpaca/medical_meadow_medical_flashcards\")\n",
    "    \n",
    "    # Select different ranges for each client\n",
    "    start_idx = (CLIENT_ID - 1) * 1000\n",
    "    end_idx = CLIENT_ID * 1000\n",
    "    small_dataset = dataset['train'].select(range(start_idx, end_idx))\n",
    "    \n",
    "    log_status(\"DATASET LOADED\", \n",
    "              f\"Dataset size: {len(small_dataset)} examples\\n\"\n",
    "              f\"Range: {start_idx} to {end_idx}\")\n",
    "    \n",
    "    # Extract test questions from the dataset\n",
    "    test_indices = np.linspace(0, len(small_dataset)-1, 5, dtype=int)\n",
    "\n",
    "    test_questions = []\n",
    "    test_answers = []\n",
    "    for idx in test_indices:\n",
    "        example = small_dataset[int(idx)]  # Convert idx to Python int\n",
    "        test_questions.append(example['input'])   # Ensure correct column names\n",
    "        test_answers.append(example['output'])    # Ensure correct column names\n",
    "    \n",
    "    log_status(\"TEST QUESTIONS SELECTED\", \n",
    "              f\"Number of test questions: {len(test_questions)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    log_status(\"DATASET ERROR\", str(e))\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the relationship between very low Mg2+ levels, PTH levels, and Ca2+ levels?',\n",
       " 'Can you describe Gardner syndrome and the conditions that it is associated with?',\n",
       " 'What is Plummer-Vinson syndrome and what are the main symptoms associated with this condition?',\n",
       " 'What is the genetic cause of cystic fibrosis, and which gene and chromosome is affected by this defect?',\n",
       " 'What is the relationship between upper motoneuron lesions and paralysis?']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\datasets\\utils\\_dill.py:385: DeprecationWarning: co_lnotab is deprecated, use co_lines instead.\n",
      "  obj.co_lnotab,  # for < python 3.10 [not counted in args]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "204e29a91e1f42ec8062a439174c02b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Format flashcards\n",
    "def format_flashcard(example):\n",
    "    return {\n",
    "        'text': f\"Question: {example['input']}\\nAnswer: {example['output']}\\n\\n\"\n",
    "    }\n",
    "\n",
    "formatted_dataset = small_dataset.map(format_flashcard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 15:13:34,774 - Client_1 - INFO - \n",
      "==================== MODEL INITIALIZATION ====================\n",
      "2025-03-09 15:13:34,776 - Client_1 - INFO - Loading model and tokenizer...\n",
      "2025-03-09 15:13:34,776 - Client_1 - INFO - ===============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== MODEL INITIALIZATION ====================\n",
      "Loading model and tokenizer...\n",
      "===============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 15:13:36,528 - Client_1 - INFO - \n",
      "==================== MODEL LOADED ====================\n",
      "2025-03-09 15:13:36,529 - Client_1 - INFO - Model: gpt2\n",
      "Parameters: 124439808\n",
      "2025-03-09 15:13:36,529 - Client_1 - INFO - =======================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== MODEL LOADED ====================\n",
      "Model: gpt2\n",
      "Parameters: 124439808\n",
      "=======================================================\n"
     ]
    }
   ],
   "source": [
    "# Initialize model and tokenizer\n",
    "log_status(\"MODEL INITIALIZATION\", \"Loading model and tokenizer...\")\n",
    "try:\n",
    "    model_name = \"gpt2\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "    \n",
    "    # Configure tokenizer\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "    \n",
    "    log_status(\"MODEL LOADED\", \n",
    "              f\"Model: {model_name}\\n\"\n",
    "              f\"Parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "except Exception as e:\n",
    "    log_status(\"MODEL ERROR\", str(e))\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate answers\n",
    "def generate_answer(question: str, max_length: int = 100) -> str:\n",
    "    try:\n",
    "        prompt = f\"Question: {question}\\nAnswer:\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            no_repeat_ngram_size=2,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating answer: {e}\")\n",
    "        return f\"Error generating answer: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate model responses\n",
    "def evaluate_model_responses(phase=\"Before\"):\n",
    "    log_status(f\"{phase.upper()} TRAINING EVALUATION\", \"Starting model evaluation...\")\n",
    "    responses = {}\n",
    "    for q, a in zip(test_questions, test_answers):\n",
    "        response = generate_answer(q)\n",
    "        responses[q] = {\n",
    "            'model_response': response,\n",
    "            'ground_truth': a\n",
    "        }\n",
    "        print(f\"\\nQuestion: {q}\")\n",
    "        print(f\"Model Response: {response}\")\n",
    "        print(f\"Ground Truth: {a}\")\n",
    "        print(\"-\" * 50)\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 15:13:36,588 - Client_1 - INFO - \n",
      "==================== TOKENIZATION ====================\n",
      "2025-03-09 15:13:36,589 - Client_1 - INFO - Tokenizing dataset...\n",
      "2025-03-09 15:13:36,590 - Client_1 - INFO - =======================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== TOKENIZATION ====================\n",
      "Tokenizing dataset...\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "003c61e279e64c5d8e1779db0294abac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "log_status(\"TOKENIZATION\", \"Tokenizing dataset...\")\n",
    "tokenized_dataset = formatted_dataset.map(\n",
    "    tokenize_function,\n",
    "    remove_columns=formatted_dataset.column_names,\n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(temp_dir, f\"client_{CLIENT_ID}\"),\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=100,\n",
    "    learning_rate=5e-5,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_total_limit=2,\n",
    "    overwrite_output_dir=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Flower client\n",
    "class MedicalFlashcardsClient(fl.client.NumPyClient):\n",
    "    def __init__(self):\n",
    "        log_status(\"CLIENT INITIALIZATION\", f\"Client {CLIENT_ID} initializing...\")\n",
    "        self.trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_dataset,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "        log_status(\"CLIENT READY\", f\"Client {CLIENT_ID} initialized and ready\")\n",
    "        \n",
    "    def get_parameters(self, config: Dict[str, str]) -> List[np.ndarray]:\n",
    "        log_status(\"PARAMETER RETRIEVAL\", f\"Client {CLIENT_ID}: Getting parameters\")\n",
    "        return [val.cpu().numpy() for _, val in model.state_dict().items()]\n",
    "    \n",
    "    def set_parameters(self, parameters: List[np.ndarray]) -> None:\n",
    "        log_status(\"PARAMETER UPDATE\", f\"Client {CLIENT_ID}: Setting parameters\")\n",
    "        params_dict = zip(model.state_dict().keys(), parameters)\n",
    "        state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
    "        model.load_state_dict(state_dict, strict=True)\n",
    "    \n",
    "    def fit(\n",
    "        self, parameters: List[np.ndarray], config: Dict[str, str]\n",
    "    ) -> Tuple[List[np.ndarray], int, Dict[str, float]]:\n",
    "        log_status(\"TRAINING START\", f\"Client {CLIENT_ID}: Starting training round\")\n",
    "        self.set_parameters(parameters)\n",
    "        self.trainer.train()\n",
    "        log_status(\"TRAINING COMPLETE\", f\"Client {CLIENT_ID}: Completed training round\")\n",
    "        return self.get_parameters(config), len(tokenized_dataset), {}\n",
    "    \n",
    "    def evaluate(\n",
    "        self, parameters: List[np.ndarray], config: Dict[str, str]\n",
    "    ) -> Tuple[float, int, Dict[str, float]]:\n",
    "        log_status(\"EVALUATION\", f\"Client {CLIENT_ID}: Evaluating model\")\n",
    "        self.set_parameters(parameters)\n",
    "        metrics = self.trainer.evaluate()\n",
    "        return float(metrics[\"eval_loss\"]), len(tokenized_dataset), {\"loss\": float(metrics[\"eval_loss\"])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model function\n",
    "def save_model(path: str = None):\n",
    "    try:\n",
    "        if path is None:\n",
    "            path = os.path.join(temp_dir, f\"medical-model-client-{CLIENT_ID}\")\n",
    "        \n",
    "        Path(path).mkdir(parents=True, exist_ok=True)\n",
    "        model.save_pretrained(path)\n",
    "        tokenizer.save_pretrained(path)\n",
    "        log_status(\"MODEL SAVED\", f\"Model saved to {path}\")\n",
    "    except Exception as e:\n",
    "        log_status(\"SAVE ERROR\", str(e))\n",
    "        # Try fallback location\n",
    "        home_dir = os.path.expanduser(\"~\")\n",
    "        fallback_path = os.path.join(home_dir, f\"medical_model_backup_client_{CLIENT_ID}\")\n",
    "        Path(fallback_path).mkdir(parents=True, exist_ok=True)\n",
    "        model.save_pretrained(fallback_path)\n",
    "        tokenizer.save_pretrained(fallback_path)\n",
    "        log_status(\"FALLBACK SAVE\", f\"Model saved to fallback location: {fallback_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 15:13:36,948 - Client_1 - INFO - \n",
      "==================== PRE-TRAINING EVALUATION ====================\n",
      "2025-03-09 15:13:36,948 - Client_1 - INFO - Testing model before training...\n",
      "2025-03-09 15:13:36,950 - Client_1 - INFO - ==================================================================\n",
      "2025-03-09 15:13:36,951 - Client_1 - INFO - \n",
      "==================== BEFORE TRAINING EVALUATION ====================\n",
      "2025-03-09 15:13:36,951 - Client_1 - INFO - Starting model evaluation...\n",
      "2025-03-09 15:13:36,951 - Client_1 - INFO - =====================================================================\n",
      "c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== PRE-TRAINING EVALUATION ====================\n",
      "Testing model before training...\n",
      "==================================================================\n",
      "\n",
      "==================== BEFORE TRAINING EVALUATION ====================\n",
      "Starting model evaluation...\n",
      "=====================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is the relationship between very low Mg2+ levels, PTH levels, and Ca2+ levels?\n",
      "Model Response: Question: What is the relationship between very low Mg2+ levels, PTH levels, and Ca2+ levels?\n",
      "Answer: The relationship is not clear. The most common explanation is that the higher the M g2 level, the more Ca 2+ is present. However, this is a very difficult question to answer.\n",
      "The most commonly accepted explanation for the correlation between M G2 and M Th is:\n",
      "Mg 2 = M + M (M + Th)\n",
      "This\n",
      "Ground Truth: Very low Mg2+ levels correspond to low PTH levels which in turn results in low Ca2+ levels.\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: Can you describe Gardner syndrome and the conditions that it is associated with?\n",
      "Model Response: Question: Can you describe Gardner syndrome and the conditions that it is associated with?\n",
      "Answer: Gardner Syndrome is a condition that occurs when a person is unable to communicate with others. It is characterized by a lack of communication and inability to understand the meaning of words.\n",
      "Gardner Syndrome can be diagnosed by looking at the eyes, nose, or throat. The symptoms of Gardner are:\n",
      "\n",
      "A lack or inability of coordination\n",
      " (a lack in coordination)\n",
      ", (A inability or unwillingness\n",
      "Ground Truth: Gardner syndrome is a medical condition that is associated with the combination of familial adenomatous polyposis, osteomas, and fibromatosis.\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is Plummer-Vinson syndrome and what are the main symptoms associated with this condition?\n",
      "Model Response: Question: What is Plummer-Vinson syndrome and what are the main symptoms associated with this condition?\n",
      "Answer: Plum-vinson is a condition that occurs when the body's immune system is weakened. It is caused by a deficiency of the immune response to the placenta. The plum is the part of your body that is responsible for producing the antibodies that protect your baby from the infection.\n",
      "Plum vinson can be caused either by the mother's own immune deficiency or\n",
      "Ground Truth: Plummer-Vinson syndrome is a medical condition characterized by iron deficiency anemia, esophageal webs, and atrophic glossitis. Some common symptoms of this syndrome include difficulty swallowing, mouth sores, and fatigue.\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is the genetic cause of cystic fibrosis, and which gene and chromosome is affected by this defect?\n",
      "Model Response: Question: What is the genetic cause of cystic fibrosis, and which gene and chromosome is affected by this defect?\n",
      "Answer: The genetic causes of the cyst are not known. However, the most common cause is a genetic mutation in the gene for the enzyme that causes the fibroblast cell to grow. The gene is responsible for producing the protein that makes cysts.\n",
      "The cysteine is an enzyme in cytoplasm that converts cytochrome P450 (CYP\n",
      "Ground Truth: Cystic fibrosis is caused by a genetic defect in the CFTR gene, which is located on chromosome 7. This gene is responsible for producing a protein that regulates the movement of salt and water in and out of cells. When the CFTR gene is defective, it leads to the production of thick, sticky mucus in the lungs, pancreas, and other organs, which can cause a range of symptoms and complications associated with cystic fibrosis.\n",
      "--------------------------------------------------\n",
      "\n",
      "Question: What is the relationship between upper motoneuron lesions and paralysis?\n",
      "Model Response: Question: What is the relationship between upper motoneuron lesions and paralysis?\n",
      "Answer: The relationship is not clear. The authors of the study reported that the lesion was caused by a single lesional lesions on the left side of a motor neuron. However, the authors also reported a lesioned lesive tissue on a right side.\n",
      "The authors concluded that there is no relationship of upper motor neurons to paralysis. They concluded: \"The lesus is a result of an abnormal lesivity\n",
      "Ground Truth: Upper motoneuron lesions may cause spastic paralysis.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Calculate similarity between responses\n",
    "def calculate_similarity(str1, str2):\n",
    "    words1 = set(str1.lower().split())\n",
    "    words2 = set(str2.lower().split())\n",
    "    overlap = len(words1.intersection(words2))\n",
    "    union = len(words1.union(words2))\n",
    "    return overlap / union if union > 0 else 0\n",
    "\n",
    "# Test before training\n",
    "log_status(\"PRE-TRAINING EVALUATION\", \"Testing model before training...\")\n",
    "before_responses = evaluate_model_responses(\"Before\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 15:13:43,360 - Client_1 - INFO - \n",
      "==================== CONNECTION SETUP ====================\n",
      "2025-03-09 15:13:43,361 - Client_1 - INFO - Starting Flower client 1\n",
      "Server address: 127.0.0.1:8081\n",
      "2025-03-09 15:13:43,361 - Client_1 - INFO - ===========================================================\n",
      "2025-03-09 15:13:43,363 - Client_1 - INFO - \n",
      "==================== CONNECTION ATTEMPT ====================\n",
      "2025-03-09 15:13:43,364 - Client_1 - INFO - Attempt 1 of 3\n",
      "2025-03-09 15:13:43,364 - Client_1 - INFO - =============================================================\n",
      "2025-03-09 15:13:43,366 - Client_1 - INFO - \n",
      "==================== CLIENT INITIALIZATION ====================\n",
      "2025-03-09 15:13:43,366 - Client_1 - INFO - Client 1 initializing...\n",
      "2025-03-09 15:13:43,367 - Client_1 - INFO - ================================================================\n",
      "2025-03-09 15:13:43,376 - Client_1 - INFO - \n",
      "==================== CLIENT READY ====================\n",
      "2025-03-09 15:13:43,377 - Client_1 - INFO - Client 1 initialized and ready\n",
      "2025-03-09 15:13:43,378 - Client_1 - INFO - =======================================================\n",
      "\u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: flwr.client.start_numpy_client() is deprecated. \n",
      "\tInstead, use `flwr.client.start_client()` by ensuring you first call the `.to_client()` method as shown below: \n",
      "\tflwr.client.start_client(\n",
      "\t\tserver_address='<IP>:<PORT>',\n",
      "\t\tclient=FlowerClient().to_client(), # <-- where FlowerClient is of type flwr.client.NumPyClient object\n",
      "\t)\n",
      "\tUsing `start_numpy_client()` is deprecated.\n",
      "\n",
      "            This is a deprecated feature. It will be removed\n",
      "            entirely in future versions of Flower.\n",
      "        \n",
      "2025-03-09 15:13:43,378 - flwr - WARNING - DEPRECATED FEATURE: flwr.client.start_numpy_client() is deprecated. \n",
      "\tInstead, use `flwr.client.start_client()` by ensuring you first call the `.to_client()` method as shown below: \n",
      "\tflwr.client.start_client(\n",
      "\t\tserver_address='<IP>:<PORT>',\n",
      "\t\tclient=FlowerClient().to_client(), # <-- where FlowerClient is of type flwr.client.NumPyClient object\n",
      "\t)\n",
      "\tUsing `start_numpy_client()` is deprecated.\n",
      "\n",
      "            This is a deprecated feature. It will be removed\n",
      "            entirely in future versions of Flower.\n",
      "        \n",
      "\u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: flwr.client.start_client() is deprecated.\n",
      "\tInstead, use the `flower-supernode` CLI command to start a SuperNode as shown below:\n",
      "\n",
      "\t\t$ flower-supernode --insecure --superlink='<IP>:<PORT>'\n",
      "\n",
      "\tTo view all available options, run:\n",
      "\n",
      "\t\t$ flower-supernode --help\n",
      "\n",
      "\tUsing `start_client()` is deprecated.\n",
      "\n",
      "            This is a deprecated feature. It will be removed\n",
      "            entirely in future versions of Flower.\n",
      "        \n",
      "2025-03-09 15:13:43,380 - flwr - WARNING - DEPRECATED FEATURE: flwr.client.start_client() is deprecated.\n",
      "\tInstead, use the `flower-supernode` CLI command to start a SuperNode as shown below:\n",
      "\n",
      "\t\t$ flower-supernode --insecure --superlink='<IP>:<PORT>'\n",
      "\n",
      "\tTo view all available options, run:\n",
      "\n",
      "\t\t$ flower-supernode --help\n",
      "\n",
      "\tUsing `start_client()` is deprecated.\n",
      "\n",
      "            This is a deprecated feature. It will be removed\n",
      "            entirely in future versions of Flower.\n",
      "        \n",
      "2025-03-09 15:13:43,382 - flwr - DEBUG - Opened insecure gRPC connection (no certificates were passed)\n",
      "2025-03-09 15:13:43,386 - flwr - DEBUG - ChannelConnectivity.IDLE\n",
      "2025-03-09 15:13:43,389 - flwr - DEBUG - ChannelConnectivity.READY\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "2025-03-09 15:13:43,389 - flwr - INFO - \n",
      "\u001b[92mINFO \u001b[0m:      Received: get_parameters message 9c56a7dc-5872-4113-89ed-a59ee00fc3c6\n",
      "2025-03-09 15:13:43,390 - flwr - INFO - Received: get_parameters message 9c56a7dc-5872-4113-89ed-a59ee00fc3c6\n",
      "2025-03-09 15:13:43,393 - Client_1 - INFO - \n",
      "==================== PARAMETER RETRIEVAL ====================\n",
      "2025-03-09 15:13:43,393 - Client_1 - INFO - Client 1: Getting parameters\n",
      "2025-03-09 15:13:43,394 - Client_1 - INFO - ==============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== CONNECTION SETUP ====================\n",
      "Starting Flower client 1\n",
      "Server address: 127.0.0.1:8081\n",
      "===========================================================\n",
      "\n",
      "==================== CONNECTION ATTEMPT ====================\n",
      "Attempt 1 of 3\n",
      "=============================================================\n",
      "\n",
      "==================== CLIENT INITIALIZATION ====================\n",
      "Client 1 initializing...\n",
      "================================================================\n",
      "\n",
      "==================== CLIENT READY ====================\n",
      "Client 1 initialized and ready\n",
      "=======================================================\n",
      "\n",
      "==================== PARAMETER RETRIEVAL ====================\n",
      "Client 1: Getting parameters\n",
      "==============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      Sent reply\n",
      "2025-03-09 15:13:44,361 - flwr - INFO - Sent reply\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "2025-03-09 15:13:58,690 - flwr - INFO - \n",
      "\u001b[92mINFO \u001b[0m:      Received: train message 08acf692-e783-4d4c-8a3d-3c054b443e5b\n",
      "2025-03-09 15:13:58,699 - flwr - INFO - Received: train message 08acf692-e783-4d4c-8a3d-3c054b443e5b\n",
      "2025-03-09 15:13:59,005 - Client_1 - INFO - \n",
      "==================== TRAINING START ====================\n",
      "2025-03-09 15:13:59,006 - Client_1 - INFO - Client 1: Starting training round\n",
      "2025-03-09 15:13:59,007 - Client_1 - INFO - =========================================================\n",
      "2025-03-09 15:13:59,008 - Client_1 - INFO - \n",
      "==================== PARAMETER UPDATE ====================\n",
      "2025-03-09 15:13:59,008 - Client_1 - INFO - Client 1: Setting parameters\n",
      "2025-03-09 15:13:59,009 - Client_1 - INFO - ===========================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== TRAINING START ====================\n",
      "Client 1: Starting training round\n",
      "=========================================================\n",
      "\n",
      "==================== PARAMETER UPDATE ====================\n",
      "Client 1: Setting parameters\n",
      "===========================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e69bf9e7cf04a54958d7e75ac48deec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/186 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8175, 'grad_norm': 10.451602935791016, 'learning_rate': 4.5e-06, 'epoch': 0.16}\n",
      "{'loss': 2.6761, 'grad_norm': 8.25395679473877, 'learning_rate': 9.5e-06, 'epoch': 0.32}\n",
      "{'loss': 2.4606, 'grad_norm': 7.498008728027344, 'learning_rate': 1.45e-05, 'epoch': 0.48}\n",
      "{'loss': 2.2761, 'grad_norm': 6.108491897583008, 'learning_rate': 1.9500000000000003e-05, 'epoch': 0.64}\n",
      "{'loss': 2.1735, 'grad_norm': 5.146684646606445, 'learning_rate': 2.45e-05, 'epoch': 0.8}\n",
      "{'loss': 2.143, 'grad_norm': 4.9028167724609375, 'learning_rate': 2.95e-05, 'epoch': 0.96}\n",
      "{'loss': 2.0158, 'grad_norm': 4.702527046203613, 'learning_rate': 3.45e-05, 'epoch': 1.12}\n",
      "{'loss': 1.929, 'grad_norm': 4.356714725494385, 'learning_rate': 3.9500000000000005e-05, 'epoch': 1.28}\n",
      "{'loss': 1.9331, 'grad_norm': 6.24608039855957, 'learning_rate': 4.4500000000000004e-05, 'epoch': 1.44}\n",
      "{'loss': 1.8212, 'grad_norm': 5.034273624420166, 'learning_rate': 4.9500000000000004e-05, 'epoch': 1.6}\n",
      "{'loss': 1.8024, 'grad_norm': 5.900687217712402, 'learning_rate': 4.476744186046512e-05, 'epoch': 1.76}\n",
      "{'loss': 1.7696, 'grad_norm': 5.6993408203125, 'learning_rate': 3.895348837209303e-05, 'epoch': 1.92}\n",
      "{'loss': 1.7025, 'grad_norm': 5.4918928146362305, 'learning_rate': 3.313953488372093e-05, 'epoch': 2.08}\n",
      "{'loss': 1.5668, 'grad_norm': 4.708030700683594, 'learning_rate': 2.7325581395348836e-05, 'epoch': 2.24}\n",
      "{'loss': 1.6168, 'grad_norm': 6.027627944946289, 'learning_rate': 2.1511627906976744e-05, 'epoch': 2.4}\n",
      "{'loss': 1.6023, 'grad_norm': 5.914601802825928, 'learning_rate': 1.569767441860465e-05, 'epoch': 2.56}\n",
      "{'loss': 1.5738, 'grad_norm': 6.36426305770874, 'learning_rate': 9.883720930232558e-06, 'epoch': 2.72}\n",
      "{'loss': 1.6106, 'grad_norm': 6.321314811706543, 'learning_rate': 4.0697674418604655e-06, 'epoch': 2.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 15:21:12,765 - Client_1 - INFO - \n",
      "==================== TRAINING COMPLETE ====================\n",
      "2025-03-09 15:21:12,765 - Client_1 - INFO - Client 1: Completed training round\n",
      "2025-03-09 15:21:12,767 - Client_1 - INFO - ============================================================\n",
      "2025-03-09 15:21:12,767 - Client_1 - INFO - \n",
      "==================== PARAMETER RETRIEVAL ====================\n",
      "2025-03-09 15:21:12,768 - Client_1 - INFO - Client 1: Getting parameters\n",
      "2025-03-09 15:21:12,768 - Client_1 - INFO - ==============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 432.6436, 'train_samples_per_second': 6.934, 'train_steps_per_second': 0.43, 'train_loss': 1.957088065403764, 'epoch': 2.98}\n",
      "\n",
      "==================== TRAINING COMPLETE ====================\n",
      "Client 1: Completed training round\n",
      "============================================================\n",
      "\n",
      "==================== PARAMETER RETRIEVAL ====================\n",
      "Client 1: Getting parameters\n",
      "==============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      Sent reply\n",
      "2025-03-09 15:21:14,610 - flwr - INFO - Sent reply\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "2025-03-09 15:21:25,013 - flwr - INFO - \n",
      "\u001b[92mINFO \u001b[0m:      Received: evaluate message b40c8516-cdd4-47ae-be18-81a4f7ae5694\n",
      "2025-03-09 15:21:25,015 - flwr - INFO - Received: evaluate message b40c8516-cdd4-47ae-be18-81a4f7ae5694\n",
      "2025-03-09 15:21:25,218 - Client_1 - INFO - \n",
      "==================== EVALUATION ====================\n",
      "2025-03-09 15:21:25,219 - Client_1 - INFO - Client 1: Evaluating model\n",
      "2025-03-09 15:21:25,220 - Client_1 - INFO - =====================================================\n",
      "2025-03-09 15:21:25,221 - Client_1 - INFO - \n",
      "==================== PARAMETER UPDATE ====================\n",
      "2025-03-09 15:21:25,223 - Client_1 - INFO - Client 1: Setting parameters\n",
      "2025-03-09 15:21:25,223 - Client_1 - INFO - ===========================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== EVALUATION ====================\n",
      "Client 1: Evaluating model\n",
      "=====================================================\n",
      "\n",
      "==================== PARAMETER UPDATE ====================\n",
      "Client 1: Setting parameters\n",
      "===========================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91mERROR \u001b[0m:     Client raised an exception.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\flwr\\client\\app.py\", line 570, in start_client_internal\n",
      "    reply_message = client_app(message=message, context=context)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\flwr\\client\\client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\flwr\\client\\client_app.py\", line 126, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\flwr\\client\\message_handler\\message_handler.py\", line 135, in handle_legacy_message_from_msgtype\n",
      "    evaluate_res = maybe_call_evaluate(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\flwr\\client\\client.py\", line 244, in maybe_call_evaluate\n",
      "    return client.evaluate(evaluate_ins)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\flwr\\client\\numpy_client.py\", line 251, in _evaluate\n",
      "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Nil Atabey\\AppData\\Local\\Temp\\ipykernel_18104\\373551083.py\", line 37, in evaluate\n",
      "    metrics = self.trainer.evaluate()\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py\", line 3861, in evaluate\n",
      "    eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py\", line 965, in get_eval_dataloader\n",
      "    raise ValueError(\"Trainer: evaluation requires an eval_dataset.\")\n",
      "ValueError: Trainer: evaluation requires an eval_dataset.\n",
      "2025-03-09 15:21:25,701 - flwr - ERROR - Client raised an exception.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\flwr\\client\\app.py\", line 570, in start_client_internal\n",
      "    reply_message = client_app(message=message, context=context)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\flwr\\client\\client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\flwr\\client\\client_app.py\", line 126, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\flwr\\client\\message_handler\\message_handler.py\", line 135, in handle_legacy_message_from_msgtype\n",
      "    evaluate_res = maybe_call_evaluate(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\flwr\\client\\client.py\", line 244, in maybe_call_evaluate\n",
      "    return client.evaluate(evaluate_ins)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\flwr\\client\\numpy_client.py\", line 251, in _evaluate\n",
      "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Nil Atabey\\AppData\\Local\\Temp\\ipykernel_18104\\373551083.py\", line 37, in evaluate\n",
      "    metrics = self.trainer.evaluate()\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py\", line 3861, in evaluate\n",
      "    eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py\", line 965, in get_eval_dataloader\n",
      "    raise ValueError(\"Trainer: evaluation requires an eval_dataset.\")\n",
      "ValueError: Trainer: evaluation requires an eval_dataset.\n",
      "2025-03-09 15:21:25,861 - flwr - DEBUG - gRPC channel closed\n",
      "2025-03-09 15:21:25,861 - Client_1 - INFO - \n",
      "==================== CONNECTION ERROR ====================\n",
      "2025-03-09 15:21:25,861 - Client_1 - INFO - Error on attempt 1: Trainer: evaluation requires an eval_dataset.\n",
      "2025-03-09 15:21:25,863 - Client_1 - INFO - ===========================================================\n",
      "2025-03-09 15:21:25,864 - Client_1 - INFO - Retrying in 5 seconds...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== CONNECTION ERROR ====================\n",
      "Error on attempt 1: Trainer: evaluation requires an eval_dataset.\n",
      "===========================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 15:21:30,906 - Client_1 - INFO - \n",
      "==================== CONNECTION ATTEMPT ====================\n",
      "2025-03-09 15:21:30,909 - Client_1 - INFO - Attempt 2 of 3\n",
      "2025-03-09 15:21:30,911 - Client_1 - INFO - =============================================================\n",
      "2025-03-09 15:21:30,913 - Client_1 - INFO - \n",
      "==================== CLIENT INITIALIZATION ====================\n",
      "2025-03-09 15:21:30,914 - Client_1 - INFO - Client 1 initializing...\n",
      "2025-03-09 15:21:30,916 - Client_1 - INFO - ================================================================\n",
      "2025-03-09 15:21:30,931 - Client_1 - INFO - \n",
      "==================== CLIENT READY ====================\n",
      "2025-03-09 15:21:30,932 - Client_1 - INFO - Client 1 initialized and ready\n",
      "2025-03-09 15:21:30,933 - Client_1 - INFO - =======================================================\n",
      "\u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: flwr.client.start_numpy_client() is deprecated. \n",
      "\tInstead, use `flwr.client.start_client()` by ensuring you first call the `.to_client()` method as shown below: \n",
      "\tflwr.client.start_client(\n",
      "\t\tserver_address='<IP>:<PORT>',\n",
      "\t\tclient=FlowerClient().to_client(), # <-- where FlowerClient is of type flwr.client.NumPyClient object\n",
      "\t)\n",
      "\tUsing `start_numpy_client()` is deprecated.\n",
      "\n",
      "            This is a deprecated feature. It will be removed\n",
      "            entirely in future versions of Flower.\n",
      "        \n",
      "2025-03-09 15:21:30,934 - flwr - WARNING - DEPRECATED FEATURE: flwr.client.start_numpy_client() is deprecated. \n",
      "\tInstead, use `flwr.client.start_client()` by ensuring you first call the `.to_client()` method as shown below: \n",
      "\tflwr.client.start_client(\n",
      "\t\tserver_address='<IP>:<PORT>',\n",
      "\t\tclient=FlowerClient().to_client(), # <-- where FlowerClient is of type flwr.client.NumPyClient object\n",
      "\t)\n",
      "\tUsing `start_numpy_client()` is deprecated.\n",
      "\n",
      "            This is a deprecated feature. It will be removed\n",
      "            entirely in future versions of Flower.\n",
      "        \n",
      "\u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: flwr.client.start_client() is deprecated.\n",
      "\tInstead, use the `flower-supernode` CLI command to start a SuperNode as shown below:\n",
      "\n",
      "\t\t$ flower-supernode --insecure --superlink='<IP>:<PORT>'\n",
      "\n",
      "\tTo view all available options, run:\n",
      "\n",
      "\t\t$ flower-supernode --help\n",
      "\n",
      "\tUsing `start_client()` is deprecated.\n",
      "\n",
      "            This is a deprecated feature. It will be removed\n",
      "            entirely in future versions of Flower.\n",
      "        \n",
      "2025-03-09 15:21:30,934 - flwr - WARNING - DEPRECATED FEATURE: flwr.client.start_client() is deprecated.\n",
      "\tInstead, use the `flower-supernode` CLI command to start a SuperNode as shown below:\n",
      "\n",
      "\t\t$ flower-supernode --insecure --superlink='<IP>:<PORT>'\n",
      "\n",
      "\tTo view all available options, run:\n",
      "\n",
      "\t\t$ flower-supernode --help\n",
      "\n",
      "\tUsing `start_client()` is deprecated.\n",
      "\n",
      "            This is a deprecated feature. It will be removed\n",
      "            entirely in future versions of Flower.\n",
      "        \n",
      "2025-03-09 15:21:30,943 - flwr - DEBUG - Opened insecure gRPC connection (no certificates were passed)\n",
      "2025-03-09 15:21:30,949 - flwr - DEBUG - ChannelConnectivity.IDLE\n",
      "2025-03-09 15:21:30,957 - flwr - DEBUG - ChannelConnectivity.READY\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== CONNECTION ATTEMPT ====================\n",
      "Attempt 2 of 3\n",
      "=============================================================\n",
      "\n",
      "==================== CLIENT INITIALIZATION ====================\n",
      "Client 1 initializing...\n",
      "================================================================\n",
      "\n",
      "==================== CLIENT READY ====================\n",
      "Client 1 initialized and ready\n",
      "=======================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      \n",
      "2025-03-09 15:21:35,007 - flwr - INFO - \n",
      "\u001b[92mINFO \u001b[0m:      Received: train message 6c51b9d5-10a7-41c0-8b29-7d27c3a0b418\n",
      "2025-03-09 15:21:35,009 - flwr - INFO - Received: train message 6c51b9d5-10a7-41c0-8b29-7d27c3a0b418\n",
      "2025-03-09 15:21:35,235 - Client_1 - INFO - \n",
      "==================== TRAINING START ====================\n",
      "2025-03-09 15:21:35,236 - Client_1 - INFO - Client 1: Starting training round\n",
      "2025-03-09 15:21:35,237 - Client_1 - INFO - =========================================================\n",
      "2025-03-09 15:21:35,238 - Client_1 - INFO - \n",
      "==================== PARAMETER UPDATE ====================\n",
      "2025-03-09 15:21:35,238 - Client_1 - INFO - Client 1: Setting parameters\n",
      "2025-03-09 15:21:35,239 - Client_1 - INFO - ===========================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== TRAINING START ====================\n",
      "Client 1: Starting training round\n",
      "=========================================================\n",
      "\n",
      "==================== PARAMETER UPDATE ====================\n",
      "Client 1: Setting parameters\n",
      "===========================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "091cdca795c149d88a68c84180193539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/186 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5404, 'grad_norm': 5.486627578735352, 'learning_rate': 5e-06, 'epoch': 0.16}\n",
      "{'loss': 1.5069, 'grad_norm': 5.991148948669434, 'learning_rate': 9.5e-06, 'epoch': 0.32}\n",
      "{'loss': 1.5088, 'grad_norm': 5.791640281677246, 'learning_rate': 1.45e-05, 'epoch': 0.48}\n",
      "{'loss': 1.4613, 'grad_norm': 6.073049545288086, 'learning_rate': 1.9500000000000003e-05, 'epoch': 0.64}\n",
      "{'loss': 1.4368, 'grad_norm': 5.626175403594971, 'learning_rate': 2.45e-05, 'epoch': 0.8}\n",
      "{'loss': 1.445, 'grad_norm': 5.616399765014648, 'learning_rate': 2.95e-05, 'epoch': 0.96}\n",
      "{'loss': 1.3616, 'grad_norm': 5.085625648498535, 'learning_rate': 3.45e-05, 'epoch': 1.12}\n",
      "{'loss': 1.342, 'grad_norm': 4.81468391418457, 'learning_rate': 3.9500000000000005e-05, 'epoch': 1.28}\n",
      "{'loss': 1.3702, 'grad_norm': 6.492756366729736, 'learning_rate': 4.4500000000000004e-05, 'epoch': 1.44}\n",
      "{'loss': 1.2872, 'grad_norm': 5.163137912750244, 'learning_rate': 4.9500000000000004e-05, 'epoch': 1.6}\n",
      "{'loss': 1.3047, 'grad_norm': 5.640829563140869, 'learning_rate': 4.476744186046512e-05, 'epoch': 1.76}\n",
      "{'loss': 1.2965, 'grad_norm': 5.491346836090088, 'learning_rate': 3.895348837209303e-05, 'epoch': 1.92}\n",
      "{'loss': 1.2577, 'grad_norm': 5.046785354614258, 'learning_rate': 3.313953488372093e-05, 'epoch': 2.08}\n",
      "{'loss': 1.1497, 'grad_norm': 4.217692852020264, 'learning_rate': 2.7325581395348836e-05, 'epoch': 2.24}\n",
      "{'loss': 1.2242, 'grad_norm': 5.686465740203857, 'learning_rate': 2.1511627906976744e-05, 'epoch': 2.4}\n",
      "{'loss': 1.2253, 'grad_norm': 5.476269245147705, 'learning_rate': 1.569767441860465e-05, 'epoch': 2.56}\n",
      "{'loss': 1.2312, 'grad_norm': 6.222469806671143, 'learning_rate': 9.883720930232558e-06, 'epoch': 2.72}\n",
      "{'loss': 1.2896, 'grad_norm': 6.2077956199646, 'learning_rate': 4.0697674418604655e-06, 'epoch': 2.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 15:25:45,122 - Client_1 - INFO - \n",
      "==================== TRAINING COMPLETE ====================\n",
      "2025-03-09 15:25:45,123 - Client_1 - INFO - Client 1: Completed training round\n",
      "2025-03-09 15:25:45,124 - Client_1 - INFO - ============================================================\n",
      "2025-03-09 15:25:45,125 - Client_1 - INFO - \n",
      "==================== PARAMETER RETRIEVAL ====================\n",
      "2025-03-09 15:25:45,125 - Client_1 - INFO - Client 1: Getting parameters\n",
      "2025-03-09 15:25:45,126 - Client_1 - INFO - ==============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 249.0954, 'train_samples_per_second': 12.044, 'train_steps_per_second': 0.747, 'train_loss': 1.342599727774179, 'epoch': 2.98}\n",
      "\n",
      "==================== TRAINING COMPLETE ====================\n",
      "Client 1: Completed training round\n",
      "============================================================\n",
      "\n",
      "==================== PARAMETER RETRIEVAL ====================\n",
      "Client 1: Getting parameters\n",
      "==============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      Sent reply\n",
      "2025-03-09 15:25:46,821 - flwr - INFO - Sent reply\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "2025-03-09 15:25:55,233 - flwr - INFO - \n",
      "\u001b[92mINFO \u001b[0m:      Received: evaluate message 51b40eb6-7bc3-408e-8eb2-7d2237004769\n",
      "2025-03-09 15:25:55,241 - flwr - INFO - Received: evaluate message 51b40eb6-7bc3-408e-8eb2-7d2237004769\n",
      "2025-03-09 15:25:55,439 - Client_1 - INFO - \n",
      "==================== EVALUATION ====================\n",
      "2025-03-09 15:25:55,440 - Client_1 - INFO - Client 1: Evaluating model\n",
      "2025-03-09 15:25:55,440 - Client_1 - INFO - =====================================================\n",
      "2025-03-09 15:25:55,441 - Client_1 - INFO - \n",
      "==================== PARAMETER UPDATE ====================\n",
      "2025-03-09 15:25:55,441 - Client_1 - INFO - Client 1: Setting parameters\n",
      "2025-03-09 15:25:55,442 - Client_1 - INFO - ===========================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== EVALUATION ====================\n",
      "Client 1: Evaluating model\n",
      "=====================================================\n",
      "\n",
      "==================== PARAMETER UPDATE ====================\n",
      "Client 1: Setting parameters\n",
      "===========================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91mERROR \u001b[0m:     Client raised an exception.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\flwr\\client\\app.py\", line 570, in start_client_internal\n",
      "    reply_message = client_app(message=message, context=context)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\flwr\\client\\client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\flwr\\client\\client_app.py\", line 126, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\flwr\\client\\message_handler\\message_handler.py\", line 135, in handle_legacy_message_from_msgtype\n",
      "    evaluate_res = maybe_call_evaluate(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\flwr\\client\\client.py\", line 244, in maybe_call_evaluate\n",
      "    return client.evaluate(evaluate_ins)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\flwr\\client\\numpy_client.py\", line 251, in _evaluate\n",
      "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Nil Atabey\\AppData\\Local\\Temp\\ipykernel_18104\\373551083.py\", line 37, in evaluate\n",
      "    metrics = self.trainer.evaluate()\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py\", line 3861, in evaluate\n",
      "    eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py\", line 965, in get_eval_dataloader\n",
      "    raise ValueError(\"Trainer: evaluation requires an eval_dataset.\")\n",
      "ValueError: Trainer: evaluation requires an eval_dataset.\n",
      "2025-03-09 15:25:55,838 - flwr - ERROR - Client raised an exception.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\flwr\\client\\app.py\", line 570, in start_client_internal\n",
      "    reply_message = client_app(message=message, context=context)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\flwr\\client\\client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\flwr\\client\\client_app.py\", line 126, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\flwr\\client\\message_handler\\message_handler.py\", line 135, in handle_legacy_message_from_msgtype\n",
      "    evaluate_res = maybe_call_evaluate(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\flwr\\client\\client.py\", line 244, in maybe_call_evaluate\n",
      "    return client.evaluate(evaluate_ins)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\flwr\\client\\numpy_client.py\", line 251, in _evaluate\n",
      "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Nil Atabey\\AppData\\Local\\Temp\\ipykernel_18104\\373551083.py\", line 37, in evaluate\n",
      "    metrics = self.trainer.evaluate()\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py\", line 3861, in evaluate\n",
      "    eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py\", line 965, in get_eval_dataloader\n",
      "    raise ValueError(\"Trainer: evaluation requires an eval_dataset.\")\n",
      "ValueError: Trainer: evaluation requires an eval_dataset.\n",
      "2025-03-09 15:25:55,917 - flwr - DEBUG - gRPC channel closed\n",
      "2025-03-09 15:25:55,918 - Client_1 - INFO - \n",
      "==================== CONNECTION ERROR ====================\n",
      "2025-03-09 15:25:55,918 - Client_1 - INFO - Error on attempt 2: Trainer: evaluation requires an eval_dataset.\n",
      "2025-03-09 15:25:55,918 - Client_1 - INFO - ===========================================================\n",
      "2025-03-09 15:25:55,920 - Client_1 - INFO - Retrying in 5 seconds...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== CONNECTION ERROR ====================\n",
      "Error on attempt 2: Trainer: evaluation requires an eval_dataset.\n",
      "===========================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 15:26:00,966 - Client_1 - INFO - \n",
      "==================== CONNECTION ATTEMPT ====================\n",
      "2025-03-09 15:26:00,967 - Client_1 - INFO - Attempt 3 of 3\n",
      "2025-03-09 15:26:00,967 - Client_1 - INFO - =============================================================\n",
      "2025-03-09 15:26:00,968 - Client_1 - INFO - \n",
      "==================== CLIENT INITIALIZATION ====================\n",
      "2025-03-09 15:26:00,970 - Client_1 - INFO - Client 1 initializing...\n",
      "2025-03-09 15:26:00,970 - Client_1 - INFO - ================================================================\n",
      "2025-03-09 15:26:01,000 - Client_1 - INFO - \n",
      "==================== CLIENT READY ====================\n",
      "2025-03-09 15:26:01,001 - Client_1 - INFO - Client 1 initialized and ready\n",
      "2025-03-09 15:26:01,002 - Client_1 - INFO - =======================================================\n",
      "\u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: flwr.client.start_numpy_client() is deprecated. \n",
      "\tInstead, use `flwr.client.start_client()` by ensuring you first call the `.to_client()` method as shown below: \n",
      "\tflwr.client.start_client(\n",
      "\t\tserver_address='<IP>:<PORT>',\n",
      "\t\tclient=FlowerClient().to_client(), # <-- where FlowerClient is of type flwr.client.NumPyClient object\n",
      "\t)\n",
      "\tUsing `start_numpy_client()` is deprecated.\n",
      "\n",
      "            This is a deprecated feature. It will be removed\n",
      "            entirely in future versions of Flower.\n",
      "        \n",
      "2025-03-09 15:26:01,003 - flwr - WARNING - DEPRECATED FEATURE: flwr.client.start_numpy_client() is deprecated. \n",
      "\tInstead, use `flwr.client.start_client()` by ensuring you first call the `.to_client()` method as shown below: \n",
      "\tflwr.client.start_client(\n",
      "\t\tserver_address='<IP>:<PORT>',\n",
      "\t\tclient=FlowerClient().to_client(), # <-- where FlowerClient is of type flwr.client.NumPyClient object\n",
      "\t)\n",
      "\tUsing `start_numpy_client()` is deprecated.\n",
      "\n",
      "            This is a deprecated feature. It will be removed\n",
      "            entirely in future versions of Flower.\n",
      "        \n",
      "\u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: flwr.client.start_client() is deprecated.\n",
      "\tInstead, use the `flower-supernode` CLI command to start a SuperNode as shown below:\n",
      "\n",
      "\t\t$ flower-supernode --insecure --superlink='<IP>:<PORT>'\n",
      "\n",
      "\tTo view all available options, run:\n",
      "\n",
      "\t\t$ flower-supernode --help\n",
      "\n",
      "\tUsing `start_client()` is deprecated.\n",
      "\n",
      "            This is a deprecated feature. It will be removed\n",
      "            entirely in future versions of Flower.\n",
      "        \n",
      "2025-03-09 15:26:01,005 - flwr - WARNING - DEPRECATED FEATURE: flwr.client.start_client() is deprecated.\n",
      "\tInstead, use the `flower-supernode` CLI command to start a SuperNode as shown below:\n",
      "\n",
      "\t\t$ flower-supernode --insecure --superlink='<IP>:<PORT>'\n",
      "\n",
      "\tTo view all available options, run:\n",
      "\n",
      "\t\t$ flower-supernode --help\n",
      "\n",
      "\tUsing `start_client()` is deprecated.\n",
      "\n",
      "            This is a deprecated feature. It will be removed\n",
      "            entirely in future versions of Flower.\n",
      "        \n",
      "2025-03-09 15:26:01,012 - flwr - DEBUG - Opened insecure gRPC connection (no certificates were passed)\n",
      "2025-03-09 15:26:01,022 - flwr - DEBUG - ChannelConnectivity.IDLE\n",
      "2025-03-09 15:26:01,028 - flwr - DEBUG - ChannelConnectivity.READY\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== CONNECTION ATTEMPT ====================\n",
      "Attempt 3 of 3\n",
      "=============================================================\n",
      "\n",
      "==================== CLIENT INITIALIZATION ====================\n",
      "Client 1 initializing...\n",
      "================================================================\n",
      "\n",
      "==================== CLIENT READY ====================\n",
      "Client 1 initialized and ready\n",
      "=======================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      \n",
      "2025-03-09 15:26:14,574 - flwr - INFO - \n",
      "\u001b[92mINFO \u001b[0m:      Received: train message 6981a5b2-f69a-441d-8f50-c9b11c318974\n",
      "2025-03-09 15:26:14,574 - flwr - INFO - Received: train message 6981a5b2-f69a-441d-8f50-c9b11c318974\n",
      "2025-03-09 15:26:14,785 - Client_1 - INFO - \n",
      "==================== TRAINING START ====================\n",
      "2025-03-09 15:26:14,785 - Client_1 - INFO - Client 1: Starting training round\n",
      "2025-03-09 15:26:14,787 - Client_1 - INFO - =========================================================\n",
      "2025-03-09 15:26:14,787 - Client_1 - INFO - \n",
      "==================== PARAMETER UPDATE ====================\n",
      "2025-03-09 15:26:14,788 - Client_1 - INFO - Client 1: Setting parameters\n",
      "2025-03-09 15:26:14,789 - Client_1 - INFO - ===========================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== TRAINING START ====================\n",
      "Client 1: Starting training round\n",
      "=========================================================\n",
      "\n",
      "==================== PARAMETER UPDATE ====================\n",
      "Client 1: Setting parameters\n",
      "===========================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf67ceb6a51144fba0b4c2e07b9d39ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/186 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1948, 'grad_norm': 5.094603538513184, 'learning_rate': 5e-06, 'epoch': 0.16}\n",
      "{'loss': 1.1588, 'grad_norm': 5.524725437164307, 'learning_rate': 1e-05, 'epoch': 0.32}\n",
      "{'loss': 1.1458, 'grad_norm': 5.313349723815918, 'learning_rate': 1.45e-05, 'epoch': 0.48}\n",
      "{'loss': 1.0994, 'grad_norm': 5.417939186096191, 'learning_rate': 1.9500000000000003e-05, 'epoch': 0.64}\n",
      "{'loss': 1.0599, 'grad_norm': 4.723526954650879, 'learning_rate': 2.45e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0571, 'grad_norm': 4.8763017654418945, 'learning_rate': 2.95e-05, 'epoch': 0.96}\n",
      "{'loss': 0.9626, 'grad_norm': 4.279062271118164, 'learning_rate': 3.45e-05, 'epoch': 1.12}\n",
      "{'loss': 0.952, 'grad_norm': 4.070964813232422, 'learning_rate': 3.9500000000000005e-05, 'epoch': 1.28}\n",
      "{'loss': 0.9731, 'grad_norm': 5.400766849517822, 'learning_rate': 4.4500000000000004e-05, 'epoch': 1.44}\n",
      "{'loss': 0.8878, 'grad_norm': 4.302569389343262, 'learning_rate': 4.9500000000000004e-05, 'epoch': 1.6}\n",
      "{'loss': 0.9198, 'grad_norm': 5.014214515686035, 'learning_rate': 4.476744186046512e-05, 'epoch': 1.76}\n",
      "{'loss': 0.936, 'grad_norm': 4.661628723144531, 'learning_rate': 3.895348837209303e-05, 'epoch': 1.92}\n",
      "{'loss': 0.9081, 'grad_norm': 4.198501110076904, 'learning_rate': 3.313953488372093e-05, 'epoch': 2.08}\n",
      "{'loss': 0.8219, 'grad_norm': 3.332520008087158, 'learning_rate': 2.7325581395348836e-05, 'epoch': 2.24}\n",
      "{'loss': 0.912, 'grad_norm': 4.886175632476807, 'learning_rate': 2.1511627906976744e-05, 'epoch': 2.4}\n",
      "{'loss': 0.9452, 'grad_norm': 4.7440948486328125, 'learning_rate': 1.569767441860465e-05, 'epoch': 2.56}\n",
      "{'loss': 0.9861, 'grad_norm': 5.586608409881592, 'learning_rate': 9.883720930232558e-06, 'epoch': 2.72}\n",
      "{'loss': 1.0617, 'grad_norm': 5.701402187347412, 'learning_rate': 4.0697674418604655e-06, 'epoch': 2.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 15:30:20,608 - Client_1 - INFO - \n",
      "==================== TRAINING COMPLETE ====================\n",
      "2025-03-09 15:30:20,608 - Client_1 - INFO - Client 1: Completed training round\n",
      "2025-03-09 15:30:20,610 - Client_1 - INFO - ============================================================\n",
      "2025-03-09 15:30:20,610 - Client_1 - INFO - \n",
      "==================== PARAMETER RETRIEVAL ====================\n",
      "2025-03-09 15:30:20,611 - Client_1 - INFO - Client 1: Getting parameters\n",
      "2025-03-09 15:30:20,611 - Client_1 - INFO - ==============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 244.7678, 'train_samples_per_second': 12.257, 'train_steps_per_second': 0.76, 'train_loss': 0.9999844335740612, 'epoch': 2.98}\n",
      "\n",
      "==================== TRAINING COMPLETE ====================\n",
      "Client 1: Completed training round\n",
      "============================================================\n",
      "\n",
      "==================== PARAMETER RETRIEVAL ====================\n",
      "Client 1: Getting parameters\n",
      "==============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      Sent reply\n",
      "2025-03-09 15:30:22,351 - flwr - INFO - Sent reply\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "2025-03-09 15:30:37,919 - flwr - INFO - \n",
      "\u001b[92mINFO \u001b[0m:      Received: evaluate message 6bab8ccc-e495-4b15-9ecf-efb6af67f242\n",
      "2025-03-09 15:30:37,920 - flwr - INFO - Received: evaluate message 6bab8ccc-e495-4b15-9ecf-efb6af67f242\n",
      "2025-03-09 15:30:38,132 - Client_1 - INFO - \n",
      "==================== EVALUATION ====================\n",
      "2025-03-09 15:30:38,132 - Client_1 - INFO - Client 1: Evaluating model\n",
      "2025-03-09 15:30:38,133 - Client_1 - INFO - =====================================================\n",
      "2025-03-09 15:30:38,134 - Client_1 - INFO - \n",
      "==================== PARAMETER UPDATE ====================\n",
      "2025-03-09 15:30:38,134 - Client_1 - INFO - Client 1: Setting parameters\n",
      "2025-03-09 15:30:38,136 - Client_1 - INFO - ===========================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== EVALUATION ====================\n",
      "Client 1: Evaluating model\n",
      "=====================================================\n",
      "\n",
      "==================== PARAMETER UPDATE ====================\n",
      "Client 1: Setting parameters\n",
      "===========================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91mERROR \u001b[0m:     Client raised an exception.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\flwr\\client\\app.py\", line 570, in start_client_internal\n",
      "    reply_message = client_app(message=message, context=context)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\flwr\\client\\client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\flwr\\client\\client_app.py\", line 126, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\flwr\\client\\message_handler\\message_handler.py\", line 135, in handle_legacy_message_from_msgtype\n",
      "    evaluate_res = maybe_call_evaluate(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\flwr\\client\\client.py\", line 244, in maybe_call_evaluate\n",
      "    return client.evaluate(evaluate_ins)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\flwr\\client\\numpy_client.py\", line 251, in _evaluate\n",
      "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Nil Atabey\\AppData\\Local\\Temp\\ipykernel_18104\\373551083.py\", line 37, in evaluate\n",
      "    metrics = self.trainer.evaluate()\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py\", line 3861, in evaluate\n",
      "    eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py\", line 965, in get_eval_dataloader\n",
      "    raise ValueError(\"Trainer: evaluation requires an eval_dataset.\")\n",
      "ValueError: Trainer: evaluation requires an eval_dataset.\n",
      "2025-03-09 15:30:38,535 - flwr - ERROR - Client raised an exception.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\flwr\\client\\app.py\", line 570, in start_client_internal\n",
      "    reply_message = client_app(message=message, context=context)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\flwr\\client\\client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\flwr\\client\\client_app.py\", line 126, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\flwr\\client\\message_handler\\message_handler.py\", line 135, in handle_legacy_message_from_msgtype\n",
      "    evaluate_res = maybe_call_evaluate(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\flwr\\client\\client.py\", line 244, in maybe_call_evaluate\n",
      "    return client.evaluate(evaluate_ins)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\flwr\\client\\numpy_client.py\", line 251, in _evaluate\n",
      "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Nil Atabey\\AppData\\Local\\Temp\\ipykernel_18104\\373551083.py\", line 37, in evaluate\n",
      "    metrics = self.trainer.evaluate()\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py\", line 3861, in evaluate\n",
      "    eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py\", line 965, in get_eval_dataloader\n",
      "    raise ValueError(\"Trainer: evaluation requires an eval_dataset.\")\n",
      "ValueError: Trainer: evaluation requires an eval_dataset.\n",
      "2025-03-09 15:30:38,631 - flwr - DEBUG - gRPC channel closed\n",
      "2025-03-09 15:30:38,633 - Client_1 - INFO - \n",
      "==================== CONNECTION ERROR ====================\n",
      "2025-03-09 15:30:38,633 - Client_1 - INFO - Error on attempt 3: Trainer: evaluation requires an eval_dataset.\n",
      "2025-03-09 15:30:38,634 - Client_1 - INFO - ===========================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== CONNECTION ERROR ====================\n",
      "Error on attempt 3: Trainer: evaluation requires an eval_dataset.\n",
      "===========================================================\n"
     ]
    }
   ],
   "source": [
    "# Start Flower client\n",
    "log_status(\"CONNECTION SETUP\", \n",
    "         f\"Starting Flower client {CLIENT_ID}\\n\"\n",
    "         f\"Server address: 127.0.0.1:8081\")\n",
    "\n",
    "connection_attempts = 0\n",
    "max_attempts = 3\n",
    "retry_delay = 5\n",
    "\n",
    "while connection_attempts < max_attempts:\n",
    "    connection_attempts += 1\n",
    "    try:\n",
    "        log_status(\"CONNECTION ATTEMPT\", f\"Attempt {connection_attempts} of {max_attempts}\")\n",
    "        \n",
    "        fl.client.start_numpy_client(\n",
    "            server_address=\"127.0.0.1:8081\",\n",
    "            client=MedicalFlashcardsClient(),\n",
    "            transport=\"grpc-bidi\",\n",
    "            grpc_max_message_length=GRPC_MAX_MESSAGE_LENGTH\n",
    "        )\n",
    "        \n",
    "        log_status(\"TRAINING SUCCESS\", \"Client completed all training rounds\")\n",
    "        break\n",
    "        \n",
    "    except ConnectionRefusedError:\n",
    "        log_status(\"CONNECTION REFUSED\", \n",
    "                  f\"Server not available (attempt {connection_attempts})\")\n",
    "        if connection_attempts < max_attempts:\n",
    "            logger.info(f\"Retrying in {retry_delay} seconds...\")\n",
    "            time.sleep(retry_delay)\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_status(\"CONNECTION ERROR\", \n",
    "                  f\"Error on attempt {connection_attempts}: {str(e)}\")\n",
    "        if connection_attempts < max_attempts:\n",
    "            logger.info(f\"Retrying in {retry_delay} seconds...\")\n",
    "            time.sleep(retry_delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 15:32:23,944 - Client_1 - INFO - \n",
      "==================== POST-TRAINING EVALUATION ====================\n",
      "2025-03-09 15:32:23,946 - Client_1 - INFO - Testing model after training...\n",
      "2025-03-09 15:32:23,947 - Client_1 - INFO - ===================================================================\n",
      "2025-03-09 15:32:23,947 - Client_1 - INFO - \n",
      "==================== AFTER TRAINING EVALUATION ====================\n",
      "2025-03-09 15:32:23,948 - Client_1 - INFO - Starting model evaluation...\n",
      "2025-03-09 15:32:23,949 - Client_1 - INFO - ====================================================================\n",
      "c:\\Users\\Nil Atabey\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== POST-TRAINING EVALUATION ====================\n",
      "Testing model after training...\n",
      "===================================================================\n",
      "\n",
      "==================== AFTER TRAINING EVALUATION ====================\n",
      "Starting model evaluation...\n",
      "====================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is the relationship between very low Mg2+ levels, PTH levels, and Ca2+ levels?\n",
      "Model Response: Question: What is the relationship between very low Mg2+ levels, PTH levels, and Ca2+ levels?\n",
      "Answer: Very low Pthal levels are associated with very high Ca+ level levels.\n",
      "\n",
      "\n",
      "Question, what is CaII+ and what are the two types of Ca-II levels that are commonly associated in relation to very Low MG2+.\n",
      "A: CaI+ is a measurement of how high the levels of calcium in the body. This is often associated\n",
      "Ground Truth: Very low Mg2+ levels correspond to low PTH levels which in turn results in low Ca2+ levels.\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: Can you describe Gardner syndrome and the conditions that it is associated with?\n",
      "Model Response: Question: Can you describe Gardner syndrome and the conditions that it is associated with?\n",
      "Answer: Gardner Syndrome is a condition characterized by the presence of a corpus callosum, which is located in the caudate nucleus of the neck. This condition is characterized as having a \"mottled-out\" appearance, with the corpus luteum located at the base of this neck region.\n",
      "\n",
      "\n",
      "Question 5: What is the name of Gardner disease, and what is its association with Huntington's\n",
      "Ground Truth: Gardner syndrome is a medical condition that is associated with the combination of familial adenomatous polyposis, osteomas, and fibromatosis.\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is Plummer-Vinson syndrome and what are the main symptoms associated with this condition?\n",
      "Model Response: Question: What is Plummer-Vinson syndrome and what are the main symptoms associated with this condition?\n",
      "Answer: Plumpy-Walker syndrome is a condition characterized by the presence of a triad of symptoms: acute (acute), chronic (coagulary), and/or delayed (intermediate). These symptoms can occur in various stages of the disease, and can range from mild to severe. The severity of these symptoms is not always known, but it is possible to have multiple episodes\n",
      "Ground Truth: Plummer-Vinson syndrome is a medical condition characterized by iron deficiency anemia, esophageal webs, and atrophic glossitis. Some common symptoms of this syndrome include difficulty swallowing, mouth sores, and fatigue.\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is the genetic cause of cystic fibrosis, and which gene and chromosome is affected by this defect?\n",
      "Model Response: Question: What is the genetic cause of cystic fibrosis, and which gene and chromosome is affected by this defect?\n",
      "Answer: Cystic fibrillation (fibrils that form a chain of DNA defects) is caused by a defect in the CFTR gene on chromosome 5q. This gene is involved in regulating cell growth and division, which results in a range of symptoms including cystitis, malabsorption, pancreatic cysts, lung damage, liver damage and rupture, among\n",
      "Ground Truth: Cystic fibrosis is caused by a genetic defect in the CFTR gene, which is located on chromosome 7. This gene is responsible for producing a protein that regulates the movement of salt and water in and out of cells. When the CFTR gene is defective, it leads to the production of thick, sticky mucus in the lungs, pancreas, and other organs, which can cause a range of symptoms and complications associated with cystic fibrosis.\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 15:32:34,154 - Client_1 - INFO - \n",
      "==================== RESULTS COMPARISON ====================\n",
      "2025-03-09 15:32:34,155 - Client_1 - INFO - Analyzing before/after performance\n",
      "2025-03-09 15:32:34,155 - Client_1 - INFO - =============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is the relationship between upper motoneuron lesions and paralysis?\n",
      "Model Response: Question: What is the relationship between upper motoneuron lesions and paralysis?\n",
      "Answer: Upper motonesurs are associated with paralysis due to the presence of a thickening of the corpus callosum.\n",
      "\n",
      "\n",
      "Question is: How is upper motor neuron paralysis caused?\n",
      "\n",
      "Answer, Upper motor neurons are caused by the degeneration of dopaminergic neurons in the substantia nigra of each muscle of an elephant's body. This degenerated neuron leads to a loss of sensation and movement in both\n",
      "Ground Truth: Upper motoneuron lesions may cause spastic paralysis.\n",
      "--------------------------------------------------\n",
      "\n",
      "==================== RESULTS COMPARISON ====================\n",
      "Analyzing before/after performance\n",
      "=============================================================\n",
      "Improvement: 7.5%\n",
      "Improvement: 0.7%\n",
      "Improvement: -0.2%\n",
      "Improvement: 0.6%\n",
      "Improvement: -1.9%\n"
     ]
    }
   ],
   "source": [
    "# Test after training\n",
    "log_status(\"POST-TRAINING EVALUATION\", \"Testing model after training...\")\n",
    "after_responses = evaluate_model_responses(\"After\")\n",
    "\n",
    "# Compare responses\n",
    "comparison_data = {\n",
    "    \"client_id\": CLIENT_ID,\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"connection_info\": {\n",
    "        \"attempts\": connection_attempts,\n",
    "        \"max_attempts\": max_attempts,\n",
    "        \"status\": \"success\" if connection_attempts < max_attempts else \"failed\"\n",
    "    },\n",
    "    \"comparisons\": []\n",
    "}\n",
    "\n",
    "log_status(\"RESULTS COMPARISON\", \"Analyzing before/after performance\")\n",
    "before_similarities = []\n",
    "after_similarities = []\n",
    "\n",
    "for q, a in zip(test_questions, test_answers):\n",
    "    before_sim = calculate_similarity(a, before_responses[q]['model_response'])\n",
    "    after_sim = calculate_similarity(a, after_responses[q]['model_response'])\n",
    "    before_similarities.append(before_sim)\n",
    "    after_similarities.append(after_sim)\n",
    "    \n",
    "    comparison = {\n",
    "        \"question\": q,\n",
    "        \"ground_truth\": a,\n",
    "        \"before\": before_responses[q]['model_response'],\n",
    "        \"after\": after_responses[q]['model_response'],\n",
    "        \"similarity_before\": before_sim,\n",
    "        \"similarity_after\": after_sim,\n",
    "        \"improvement\": after_sim - before_sim\n",
    "    }\n",
    "    comparison_data[\"comparisons\"].append(comparison)\n",
    "    \n",
    "    print(f\"Improvement: {(after_sim - before_sim) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall metrics\n",
    "avg_improvement = np.mean(np.array(after_similarities) - np.array(before_similarities))\n",
    "comparison_data[\"metrics\"] = {\n",
    "    \"average_similarity_before\": float(np.mean(before_similarities)),\n",
    "    \"average_similarity_after\": float(np.mean(after_similarities)),\n",
    "    \"average_improvement\": float(avg_improvement),\n",
    "    \"max_improvement\": float(np.max(np.array(after_similarities) - np.array(before_similarities))),\n",
    "    \"min_improvement\": float(np.min(np.array(after_similarities) - np.array(before_similarities)))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 15:33:58,811 - Client_1 - INFO - \n",
      "==================== FINAL STATISTICS ====================\n",
      "2025-03-09 15:33:58,812 - Client_1 - INFO - Client 1 Training Summary\n",
      "Total examples: 1000\n",
      "Device: cuda\n",
      "Model parameters: 124439808\n",
      "Average improvement: 1.3%\n",
      "Results saved: C:\\Users\\NILATA~1\\AppData\\Local\\Temp\\tmp9bgt6qn4\\client_1_results.json\n",
      "2025-03-09 15:33:58,813 - Client_1 - INFO - ===========================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== FINAL STATISTICS ====================\n",
      "Client 1 Training Summary\n",
      "Total examples: 1000\n",
      "Device: cuda\n",
      "Model parameters: 124439808\n",
      "Average improvement: 1.3%\n",
      "Results saved: C:\\Users\\NILATA~1\\AppData\\Local\\Temp\\tmp9bgt6qn4\\client_1_results.json\n",
      "===========================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 15:33:59,816 - Client_1 - INFO - \n",
      "==================== MODEL SAVED ====================\n",
      "2025-03-09 15:33:59,816 - Client_1 - INFO - Model saved to C:\\Users\\NILATA~1\\AppData\\Local\\Temp\\tmp9bgt6qn4\\medical-model-client-1\n",
      "2025-03-09 15:33:59,818 - Client_1 - INFO - ======================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== MODEL SAVED ====================\n",
      "Model saved to C:\\Users\\NILATA~1\\AppData\\Local\\Temp\\tmp9bgt6qn4\\medical-model-client-1\n",
      "======================================================\n"
     ]
    }
   ],
   "source": [
    "# Save results\n",
    "results_path = os.path.join(temp_dir, f\"client_{CLIENT_ID}_results.json\")\n",
    "with open(results_path, \"w\") as f:\n",
    "    json.dump(comparison_data, f, indent=2)\n",
    "\n",
    "log_status(\"FINAL STATISTICS\", \n",
    "         f\"Client {CLIENT_ID} Training Summary\\n\"\n",
    "         f\"Total examples: {len(small_dataset)}\\n\"\n",
    "         f\"Device: {device}\\n\"\n",
    "         f\"Model parameters: {sum(p.numel() for p in model.parameters())}\\n\"\n",
    "         f\"Average improvement: {avg_improvement * 100:.1f}%\\n\"\n",
    "         f\"Results saved: {results_path}\")\n",
    "\n",
    "# Save final model\n",
    "save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
