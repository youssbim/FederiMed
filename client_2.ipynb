{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "import flwr as fl\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set client ID (change for each client: 1,2,3,4)\n",
    "CLIENT_ID = 2  # Change this for each client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(f\"Client_{CLIENT_ID}\")\n",
    "\n",
    "def log_status(status: str, details: str = \"\"):\n",
    "    status_line = f\"\\n{'='*20} {status} {'='*20}\"\n",
    "    logger.info(status_line)\n",
    "    if details:\n",
    "        logger.info(details)\n",
    "    logger.info(\"=\"*len(status_line))\n",
    "    print(status_line)\n",
    "    if details:\n",
    "        print(details)\n",
    "    print(\"=\"*len(status_line))\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(42 + CLIENT_ID)\n",
    "\n",
    "# Check CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "log_status(\"DEVICE INFO\", f\"Client {CLIENT_ID} using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary directory for outputs\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "log_status(\"DIRECTORY INFO\", f\"Using temporary directory: {temp_dir}\")\n",
    "\n",
    "# Define message size\n",
    "GRPC_MAX_MESSAGE_LENGTH = 1024 * 1024 * 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "log_status(\"LOADING DATASET\", f\"Client {CLIENT_ID} loading dataset...\")\n",
    "try:\n",
    "    dataset = load_dataset(\"medalpaca/medical_meadow_medical_flashcards\")\n",
    "    \n",
    "    # Select different ranges for each client\n",
    "    start_idx = (CLIENT_ID - 1) * 50\n",
    "    end_idx = CLIENT_ID * 50\n",
    "    small_dataset = dataset['train'].select(range(start_idx, end_idx))\n",
    "    \n",
    "    log_status(\"DATASET LOADED\", \n",
    "              f\"Dataset size: {len(small_dataset)} examples\\n\"\n",
    "              f\"Range: {start_idx} to {end_idx}\")\n",
    "    \n",
    "    # Extract test questions from the dataset\n",
    "    test_indices = np.linspace(0, len(small_dataset)-1, 5, dtype=int)\n",
    "\n",
    "    test_questions = []\n",
    "    test_answers = []\n",
    "    for idx in test_indices:\n",
    "        example = small_dataset[int(idx)]  # Convert idx to Python int\n",
    "        test_questions.append(example['input'])   # Ensure correct column names\n",
    "        test_answers.append(example['output'])    # Ensure correct column names\n",
    "    \n",
    "    log_status(\"TEST QUESTIONS SELECTED\", \n",
    "              f\"Number of test questions: {len(test_questions)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    log_status(\"DATASET ERROR\", str(e))\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format flashcards\n",
    "def format_flashcard(example):\n",
    "    return {\n",
    "        'text': f\"Question: {example['input']}\\nAnswer: {example['output']}\\n\\n\"\n",
    "    }\n",
    "\n",
    "formatted_dataset = small_dataset.map(format_flashcard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and tokenizer\n",
    "log_status(\"MODEL INITIALIZATION\", \"Loading model and tokenizer...\")\n",
    "try:\n",
    "    model_name = \"gpt2\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "    \n",
    "    # Configure tokenizer\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "    \n",
    "    log_status(\"MODEL LOADED\", \n",
    "              f\"Model: {model_name}\\n\"\n",
    "              f\"Parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "except Exception as e:\n",
    "    log_status(\"MODEL ERROR\", str(e))\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate answers\n",
    "def generate_answer(question: str, max_length: int = 100) -> str:\n",
    "    try:\n",
    "        prompt = f\"Question: {question}\\nAnswer:\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            no_repeat_ngram_size=2,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating answer: {e}\")\n",
    "        return f\"Error generating answer: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate model responses\n",
    "def evaluate_model_responses(phase=\"Before\"):\n",
    "    log_status(f\"{phase.upper()} TRAINING EVALUATION\", \"Starting model evaluation...\")\n",
    "    responses = {}\n",
    "    for q, a in zip(test_questions, test_answers):\n",
    "        response = generate_answer(q)\n",
    "        responses[q] = {\n",
    "            'model_response': response,\n",
    "            'ground_truth': a\n",
    "        }\n",
    "        print(f\"\\nQuestion: {q}\")\n",
    "        print(f\"Model Response: {response}\")\n",
    "        print(f\"Ground Truth: {a}\")\n",
    "        print(\"-\" * 50)\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "log_status(\"TOKENIZATION\", \"Tokenizing dataset...\")\n",
    "tokenized_dataset = formatted_dataset.map(\n",
    "    tokenize_function,\n",
    "    remove_columns=formatted_dataset.column_names,\n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(temp_dir, f\"client_{CLIENT_ID}\"),\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=100,\n",
    "    learning_rate=5e-5,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_total_limit=2,\n",
    "    overwrite_output_dir=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Flower client\n",
    "class MedicalFlashcardsClient(fl.client.NumPyClient):\n",
    "    def __init__(self):\n",
    "        log_status(\"CLIENT INITIALIZATION\", f\"Client {CLIENT_ID} initializing...\")\n",
    "        self.trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_dataset,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "        log_status(\"CLIENT READY\", f\"Client {CLIENT_ID} initialized and ready\")\n",
    "        \n",
    "    def get_parameters(self, config: Dict[str, str]) -> List[np.ndarray]:\n",
    "        log_status(\"PARAMETER RETRIEVAL\", f\"Client {CLIENT_ID}: Getting parameters\")\n",
    "        return [val.cpu().numpy() for _, val in model.state_dict().items()]\n",
    "    \n",
    "    def set_parameters(self, parameters: List[np.ndarray]) -> None:\n",
    "        log_status(\"PARAMETER UPDATE\", f\"Client {CLIENT_ID}: Setting parameters\")\n",
    "        params_dict = zip(model.state_dict().keys(), parameters)\n",
    "        state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
    "        model.load_state_dict(state_dict, strict=True)\n",
    "    \n",
    "    def fit(\n",
    "        self, parameters: List[np.ndarray], config: Dict[str, str]\n",
    "    ) -> Tuple[List[np.ndarray], int, Dict[str, float]]:\n",
    "        log_status(\"TRAINING START\", f\"Client {CLIENT_ID}: Starting training round\")\n",
    "        self.set_parameters(parameters)\n",
    "        self.trainer.train()\n",
    "        log_status(\"TRAINING COMPLETE\", f\"Client {CLIENT_ID}: Completed training round\")\n",
    "        return self.get_parameters(config), len(tokenized_dataset), {}\n",
    "    \n",
    "    def evaluate(\n",
    "        self, parameters: List[np.ndarray], config: Dict[str, str]\n",
    "    ) -> Tuple[float, int, Dict[str, float]]:\n",
    "        log_status(\"EVALUATION\", f\"Client {CLIENT_ID}: Evaluating model\")\n",
    "        self.set_parameters(parameters)\n",
    "        metrics = self.trainer.evaluate()\n",
    "        return float(metrics[\"eval_loss\"]), len(tokenized_dataset), {\"loss\": float(metrics[\"eval_loss\"])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model function\n",
    "def save_model(path: str = None):\n",
    "    try:\n",
    "        if path is None:\n",
    "            path = os.path.join(temp_dir, f\"medical-model-client-{CLIENT_ID}\")\n",
    "        \n",
    "        Path(path).mkdir(parents=True, exist_ok=True)\n",
    "        model.save_pretrained(path)\n",
    "        tokenizer.save_pretrained(path)\n",
    "        log_status(\"MODEL SAVED\", f\"Model saved to {path}\")\n",
    "    except Exception as e:\n",
    "        log_status(\"SAVE ERROR\", str(e))\n",
    "        # Try fallback location\n",
    "        home_dir = os.path.expanduser(\"~\")\n",
    "        fallback_path = os.path.join(home_dir, f\"medical_model_backup_client_{CLIENT_ID}\")\n",
    "        Path(fallback_path).mkdir(parents=True, exist_ok=True)\n",
    "        model.save_pretrained(fallback_path)\n",
    "        tokenizer.save_pretrained(fallback_path)\n",
    "        log_status(\"FALLBACK SAVE\", f\"Model saved to fallback location: {fallback_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarity between responses\n",
    "def calculate_similarity(str1, str2):\n",
    "    words1 = set(str1.lower().split())\n",
    "    words2 = set(str2.lower().split())\n",
    "    overlap = len(words1.intersection(words2))\n",
    "    union = len(words1.union(words2))\n",
    "    return overlap / union if union > 0 else 0\n",
    "\n",
    "# Test before training\n",
    "log_status(\"PRE-TRAINING EVALUATION\", \"Testing model before training...\")\n",
    "before_responses = evaluate_model_responses(\"Before\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Flower client\n",
    "log_status(\"CONNECTION SETUP\", \n",
    "         f\"Starting Flower client {CLIENT_ID}\\n\"\n",
    "         f\"Server address: 127.0.0.1:8081\")\n",
    "\n",
    "connection_attempts = 0\n",
    "max_attempts = 3\n",
    "retry_delay = 5\n",
    "\n",
    "while connection_attempts < max_attempts:\n",
    "    connection_attempts += 1\n",
    "    try:\n",
    "        log_status(\"CONNECTION ATTEMPT\", f\"Attempt {connection_attempts} of {max_attempts}\")\n",
    "        \n",
    "        fl.client.start_numpy_client(\n",
    "            server_address=\"127.0.0.1:8081\",\n",
    "            client=MedicalFlashcardsClient(),\n",
    "            transport=\"grpc-bidi\",\n",
    "            grpc_max_message_length=GRPC_MAX_MESSAGE_LENGTH\n",
    "        )\n",
    "        \n",
    "        log_status(\"TRAINING SUCCESS\", \"Client completed all training rounds\")\n",
    "        break\n",
    "        \n",
    "    except ConnectionRefusedError:\n",
    "        log_status(\"CONNECTION REFUSED\", \n",
    "                  f\"Server not available (attempt {connection_attempts})\")\n",
    "        if connection_attempts < max_attempts:\n",
    "            logger.info(f\"Retrying in {retry_delay} seconds...\")\n",
    "            time.sleep(retry_delay)\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_status(\"CONNECTION ERROR\", \n",
    "                  f\"Error on attempt {connection_attempts}: {str(e)}\")\n",
    "        if connection_attempts < max_attempts:\n",
    "            logger.info(f\"Retrying in {retry_delay} seconds...\")\n",
    "            time.sleep(retry_delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test after training\n",
    "log_status(\"POST-TRAINING EVALUATION\", \"Testing model after training...\")\n",
    "after_responses = evaluate_model_responses(\"After\")\n",
    "\n",
    "# Compare responses\n",
    "comparison_data = {\n",
    "    \"client_id\": CLIENT_ID,\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"connection_info\": {\n",
    "        \"attempts\": connection_attempts,\n",
    "        \"max_attempts\": max_attempts,\n",
    "        \"status\": \"success\" if connection_attempts < max_attempts else \"failed\"\n",
    "    },\n",
    "    \"comparisons\": []\n",
    "}\n",
    "\n",
    "log_status(\"RESULTS COMPARISON\", \"Analyzing before/after performance\")\n",
    "before_similarities = []\n",
    "after_similarities = []\n",
    "\n",
    "for q, a in zip(test_questions, test_answers):\n",
    "    before_sim = calculate_similarity(a, before_responses[q]['model_response'])\n",
    "    after_sim = calculate_similarity(a, after_responses[q]['model_response'])\n",
    "    before_similarities.append(before_sim)\n",
    "    after_similarities.append(after_sim)\n",
    "    \n",
    "    comparison = {\n",
    "        \"question\": q,\n",
    "        \"ground_truth\": a,\n",
    "        \"before\": before_responses[q]['model_response'],\n",
    "        \"after\": after_responses[q]['model_response'],\n",
    "        \"similarity_before\": before_sim,\n",
    "        \"similarity_after\": after_sim,\n",
    "        \"improvement\": after_sim - before_sim\n",
    "    }\n",
    "    comparison_data[\"comparisons\"].append(comparison)\n",
    "    \n",
    "    print(f\"\\nQuestion: {q}\")\n",
    "    print(f\"Ground Truth: {a}\")\n",
    "    print(f\"Before: {before_responses[q]['model_response']}\")\n",
    "    print(f\"After: {after_responses[q]['model_response']}\")\n",
    "    print(f\"Improvement: {(after_sim - before_sim) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall metrics\n",
    "avg_improvement = np.mean(np.array(after_similarities) - np.array(before_similarities))\n",
    "comparison_data[\"metrics\"] = {\n",
    "    \"average_similarity_before\": float(np.mean(before_similarities)),\n",
    "    \"average_similarity_after\": float(np.mean(after_similarities)),\n",
    "    \"average_improvement\": float(avg_improvement),\n",
    "    \"max_improvement\": float(np.max(np.array(after_similarities) - np.array(before_similarities))),\n",
    "    \"min_improvement\": float(np.min(np.array(after_similarities) - np.array(before_similarities)))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_path = os.path.join(temp_dir, f\"client_{CLIENT_ID}_results.json\")\n",
    "with open(results_path, \"w\") as f:\n",
    "    json.dump(comparison_data, f, indent=2)\n",
    "\n",
    "log_status(\"FINAL STATISTICS\", \n",
    "         f\"Client {CLIENT_ID} Training Summary\\n\"\n",
    "         f\"Total examples: {len(small_dataset)}\\n\"\n",
    "         f\"Device: {device}\\n\"\n",
    "         f\"Model parameters: {sum(p.numel() for p in model.parameters())}\\n\"\n",
    "         f\"Average improvement: {avg_improvement * 100:.1f}%\\n\"\n",
    "         f\"Results saved: {results_path}\")\n",
    "\n",
    "# Save final model\n",
    "save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
